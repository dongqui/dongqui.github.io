---
layout: post
title: "가슴으로 받아들이는 딥러닝(2)"
tags: [DL]
---
머리로 이해하기 어려운 온갖 수학식과 어려운 전문 용어를 걷어내고, 그냥 그렇구나~ 하고 가슴으로 딥러닝을 받아들여보자.

입력층으로 들어와 은닉층의 노드들의 가중치를 합산하며 여차저차 출력층 까지 도달해 결과 값을 도출했다면?<br>
우리가 데이터에 라벨링해둔 정답과 비교해본다.<br>
그렇게 모든 데이터의 결과값 오차를 차곡차곡 모아 그래프로 나타내보면 요렇게 표현 될 수 있다.
{% include image.html path="postimages/deep_2_dif" path-detail="postimages/deep_2_dif" alt="" %}
가중치가 커지거나 작아질 때 오차가 커지다가 어느 시점에서 오차가 가장 낮은 지점이 있는 것을 볼 수 있다.<br><br><br>
오차가 가장 낮은 지점으로 어떻게 찾아 갈 수 있을까?
{% include image.html path="postimages/deep_2_gradient" path-detail="postimages/deep_2_gradient" alt="" %}

각 지점의 기울기를 구하면 된다 (미분).<br>
기울기가 음수인지 양수인지에 따라 오른쪽으로 이동할 것인지 왼쪽으로 이동할 것인지 정하고,
기울기의 크기에 따라 내려갈 것인지 정한다.<br> 
모델의 목표는 기울기가 0인 지점!<br>
그렇게 조금조금씩 경사를 살피며 최적의 지점으로 하강한다(경사하강법).<br>
이 과정을 출력층 -> 인풋층 방향으로 계속해서 진행한다.
{% include image.html path="postimages/deep_2_back" path-detail="postimages/deep_2_back" alt="" %}


그렇게 모델을 혼내키고, 가중치를 수정하는 것을 반복하다보면, 처음에는 의미 없던 가중치들이 최적화 되어간다.
{% include image.html path="postimages/deep_2_repeat" path-detail="postimages/deep_2_repeat" alt="" %}
